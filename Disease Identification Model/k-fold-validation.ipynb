{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5441978,"sourceType":"datasetVersion","datasetId":3146821}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport shutil\n\nSOURCE_BASE = \"/kaggle/input/plant-village-dataset-updated\"\n\nDEST_BASE = \"/kaggle/working/plant-village-merged\"\n\nplants = [\n    \"Apple\",\n    \"Bell Pepper\",\n    \"Cherry\",\n    \"Corn (Maize)\",\n    \"Grape\",\n    \"Peach\",\n    \"Potato\",\n    \"Strawberry\",\n    \"Tomato\"\n]\n\nsplits = [\"Train\", \"Val\", \"Test\"]\n\n\nfor split in splits:\n    os.makedirs(os.path.join(DEST_BASE, split), exist_ok=True)\n\n\nfor plant in plants:\n    for split in splits:\n        plant_split_dir = os.path.join(SOURCE_BASE, plant, split)\n        if not os.path.isdir(plant_split_dir):\n            continue\n        \n        # Each disease folder inside Apple/Train, Apple/Val, etc.\n        for disease in os.listdir(plant_split_dir):\n            disease_folder = os.path.join(plant_split_dir, disease)\n            if not os.path.isdir(disease_folder):\n                continue\n            \n            # Make a combined class name to avoid collisions:\n            # e.g., \"Apple_Apple Scab\"\n            combined_class_name = f\"{plant}_{disease}\"\n            \n            # Create the destination subfolder\n            dest_class_dir = os.path.join(DEST_BASE, split, combined_class_name)\n            os.makedirs(dest_class_dir, exist_ok=True)\n            \n            # Copy all images\n            for img_name in os.listdir(disease_folder):\n                src_path = os.path.join(disease_folder, img_name)\n\n                if os.path.isdir(src_path):\n                    continue\n                \n                dst_path = os.path.join(dest_class_dir, img_name)\n                # Copy or move the image (copy2 preserves metadata)\n                shutil.copy2(src_path, dst_path)\n\nprint(\"Merging complete!\")\n\n\nimport os\n\ndef count_images_in_folder(folder_path):\n    \"\"\"Count the number of image files in a folder (recursively).\"\"\"\n    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n    count = 0\n    for root, dirs, files in os.walk(folder_path):\n        for filename in files:\n            if filename.lower().endswith(valid_extensions):\n                count += 1\n    return count\n\n# Paths to your flattened dataset splits\ntrain_path = '/kaggle/working/plant-village-merged/Train'\nval_path   = '/kaggle/working/plant-village-merged/Val'\ntest_path  = '/kaggle/working/plant-village-merged/Test'\n\n# Count images in each split\ntrain_count = count_images_in_folder(train_path)\nval_count   = count_images_in_folder(val_path)\ntest_count  = count_images_in_folder(test_path)\n\nprint(\"Total training images:  \", train_count)\nprint(\"Total validation images:\", val_count)\nprint(\"Total testing images:   \", test_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T14:50:45.675224Z","iopub.execute_input":"2025-04-22T14:50:45.675451Z","iopub.status.idle":"2025-04-22T15:00:09.719238Z","shell.execute_reply.started":"2025-04-22T14:50:45.675433Z","shell.execute_reply":"2025-04-22T15:00:09.718591Z"}},"outputs":[{"name":"stdout","text":"Merging complete!\nTotal training images:   53690\nTotal validation images: 12067\nTotal testing images:    1354\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom collections import Counter\n\n# -----------------------------\n# DATASET SETUP\n# -----------------------------\ndataset_path = '/kaggle/working/plant-village-merged'\ntrain_path = os.path.join(dataset_path, \"Train\")\nval_path   = os.path.join(dataset_path, \"Val\")\ntest_path  = os.path.join(dataset_path, \"Test\")\n\n# Function to get file paths and labels from the directory structure.\ndef get_file_paths_and_labels(train_path):\n    classes = sorted(os.listdir(train_path))\n    file_paths = []\n    labels = []\n    for idx, cls in enumerate(classes):\n        class_dir = os.path.join(train_path, cls)\n        if os.path.isdir(class_dir):\n            for file in os.listdir(class_dir):\n                if os.path.isfile(os.path.join(class_dir, file)):\n                    file_paths.append(os.path.join(class_dir, file))\n                    labels.append(idx)\n    return file_paths, labels, classes\n\n# Get the file paths and labels\nfile_paths, labels, classes = get_file_paths_and_labels(train_path)\n\n# Check counts per class (using numeric labels)\ncounts = Counter(labels)\nprint(\"Original class counts:\")\nfor cls_idx, count in counts.items():\n    print(f\"{classes[cls_idx]}: {count} images\")\n\n# Determine the maximum number of images among all classes\nmax_count = max(counts.values())\n\n# Oversample: for each class, randomly choose additional indices to match the max count.\nbalanced_file_paths = list(file_paths)\nbalanced_labels = list(labels)\n\nfor cls in range(len(classes)):\n    # Get indices for the current class\n    cls_indices = [i for i, label in enumerate(labels) if label == cls]\n    current_count = counts[cls]\n    \n    # Number of samples to add\n    n_to_add = max_count - current_count\n    if n_to_add > 0:\n        # Randomly sample indices with replacement from current indices\n        oversampled_indices = np.random.choice(cls_indices, size=n_to_add, replace=True)\n        balanced_file_paths.extend([file_paths[i] for i in oversampled_indices])\n        balanced_labels.extend([labels[i] for i in oversampled_indices])\n\n# Verify new counts\nbalanced_counts = Counter(balanced_labels)\nprint(\"\\nBalanced class counts:\")\nfor cls_idx, count in balanced_counts.items():\n    print(f\"{classes[cls_idx]}: {count} images\")\n\n# Create a TensorFlow dataset from the balanced file paths and labels.\ndef load_and_preprocess_image(path, label, img_size=(128, 128)):\n    # Read image file\n    image = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, img_size)\n    # Normalize the image to [0,1]\n    image = image / 255.0\n    return image, label\n\nbalanced_ds = tf.data.Dataset.from_tensor_slices((balanced_file_paths, balanced_labels))\nbalanced_ds = balanced_ds.shuffle(buffer_size=len(balanced_file_paths))\nbalanced_ds = balanced_ds.map(lambda path, label: load_and_preprocess_image(path, label),\n                              num_parallel_calls=tf.data.AUTOTUNE)\nbalanced_ds = balanced_ds.batch(32).prefetch(tf.data.AUTOTUNE)\n\n\n# Path where the new balanced dataset will be saved\nbalanced_dataset_path = \"/kaggle/working/balanced-plant-village\"\n\n# Create the new dataset directory\nos.makedirs(balanced_dataset_path, exist_ok=True)\n\nfor cls in classes:\n    os.makedirs(os.path.join(balanced_dataset_path, cls), exist_ok=True)\n\n# Copy original images to new dataset\nfor src_path, label in zip(file_paths, labels):\n    class_name = classes[label]\n    dst_path = os.path.join(balanced_dataset_path, class_name, os.path.basename(src_path))\n    shutil.copy(src_path, dst_path)\n\n# Oversampling: copy additional images to balance dataset\nfor cls in range(len(classes)):\n    cls_indices = [i for i, label in enumerate(labels) if label == cls]\n    current_count = counts[cls]\n    n_to_add = max_count - current_count\n    \n    if n_to_add > 0:\n        oversampled_indices = np.random.choice(cls_indices, size=n_to_add, replace=True)\n        for idx in oversampled_indices:\n            src_path = file_paths[idx]\n            class_name = classes[cls]\n            filename = f\"aug_{np.random.randint(10000)}_{os.path.basename(src_path)}\"  # Rename to avoid conflicts\n            dst_path = os.path.join(balanced_dataset_path, class_name, filename)\n            shutil.copy(src_path, dst_path)\n\nprint(f\"Balanced dataset saved at: {balanced_dataset_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T15:00:13.031086Z","iopub.execute_input":"2025-04-22T15:00:13.031835Z","iopub.status.idle":"2025-04-22T15:00:35.956094Z","shell.execute_reply.started":"2025-04-22T15:00:13.031804Z","shell.execute_reply":"2025-04-22T15:00:35.955417Z"}},"outputs":[{"name":"stderr","text":"2025-04-22 15:00:15.170518: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745334015.426349      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745334015.496737      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Original class counts:\nApple_Apple Scab: 2016 images\nApple_Black Rot: 1987 images\nApple_Cedar Apple Rust: 1760 images\nApple_Healthy: 2008 images\nBell Pepper_Bacterial Spot: 1913 images\nBell Pepper_Healthy: 1988 images\nCherry_Healthy: 1826 images\nCherry_Powdery Mildew: 1683 images\nCorn (Maize)_Cercospora Leaf Spot: 1642 images\nCorn (Maize)_Common Rust : 1907 images\nCorn (Maize)_Healthy: 1859 images\nCorn (Maize)_Northern Leaf Blight: 1908 images\nGrape_Black Rot: 1888 images\nGrape_Esca (Black Measles): 1920 images\nGrape_Healthy: 1692 images\nGrape_Leaf Blight: 1722 images\nPeach_Bacterial Spot: 1838 images\nPeach_Healthy: 1728 images\nPotato_Early Blight: 1939 images\nPotato_Healthy: 1824 images\nPotato_Late Blight: 1939 images\nStrawberry_Healthy: 1824 images\nStrawberry_Leaf Scorch: 1774 images\nTomato_Bacterial Spot: 1702 images\nTomato_Early Blight: 1920 images\nTomato_Healthy: 1926 images\nTomato_Late Blight: 1851 images\nTomato_Septoria Leaf Spot: 1745 images\nTomato_Yellow Leaf Curl Virus: 1961 images\n\nBalanced class counts:\nApple_Apple Scab: 2016 images\nApple_Black Rot: 2016 images\nApple_Cedar Apple Rust: 2016 images\nApple_Healthy: 2016 images\nBell Pepper_Bacterial Spot: 2016 images\nBell Pepper_Healthy: 2016 images\nCherry_Healthy: 2016 images\nCherry_Powdery Mildew: 2016 images\nCorn (Maize)_Cercospora Leaf Spot: 2016 images\nCorn (Maize)_Common Rust : 2016 images\nCorn (Maize)_Healthy: 2016 images\nCorn (Maize)_Northern Leaf Blight: 2016 images\nGrape_Black Rot: 2016 images\nGrape_Esca (Black Measles): 2016 images\nGrape_Healthy: 2016 images\nGrape_Leaf Blight: 2016 images\nPeach_Bacterial Spot: 2016 images\nPeach_Healthy: 2016 images\nPotato_Early Blight: 2016 images\nPotato_Healthy: 2016 images\nPotato_Late Blight: 2016 images\nStrawberry_Healthy: 2016 images\nStrawberry_Leaf Scorch: 2016 images\nTomato_Bacterial Spot: 2016 images\nTomato_Early Blight: 2016 images\nTomato_Healthy: 2016 images\nTomato_Late Blight: 2016 images\nTomato_Septoria Leaf Spot: 2016 images\nTomato_Yellow Leaf Curl Virus: 2016 images\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745334029.414304      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745334029.414954      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"Balanced dataset saved at: /kaggle/working/balanced-plant-village\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom collections import Counter\nfrom sklearn.model_selection import StratifiedKFold\nfrom tensorflow.keras import layers, Model, Input, regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# -----------------------------\n# 0. SETTINGS\n# -----------------------------\nSEED       = 42\nIMG_SIZE   = (128,128)\nBATCH_SIZE = 32\nEPOCHS     = 50\nN_SPLITS   = 5\nPROJ_DIM   = 1280\nINITIAL_LR = 1e-3\n\n# Paths (after merging & oversampling into disk)\nBALANCED_DIR = \"/kaggle/working/balanced-plant-village\"\n\n# -----------------------------\n# 1. GATHER ALL FILE PATHS & LABELS\n# -----------------------------\nclasses = sorted(os.listdir(BALANCED_DIR))\nfile_paths = []\nlabels     = []\nfor idx, cls in enumerate(classes):\n    cls_dir = os.path.join(BALANCED_DIR, cls)\n    for fname in os.listdir(cls_dir):\n        if fname.lower().endswith(('.jpg','jpeg','png')):\n            file_paths.append(os.path.join(cls_dir, fname))\n            labels.append(idx)\n\nfile_paths = np.array(file_paths)\nlabels     = np.array(labels)\nprint(\"Total images:\", len(file_paths))\nprint(\"Classes:\", classes)\n\n# -----------------------------\n# 2. HELPERS TO BUILD MODELS\n# -----------------------------\ndef build_teacher_model(base_fn):\n    base_model = base_fn(input_shape=(*IMG_SIZE,3), include_top=False, pooling='avg', weights='imagenet')\n    inp = Input(shape=(*IMG_SIZE,3))\n    base = base_model(inp)\n    proj = layers.Dense(PROJ_DIM, activation='relu', name='proj_features')(base)\n    pred = layers.Dense(len(classes), activation='softmax')(proj)\n    return Model(inp, [proj, pred])\n\ndef squeeze_excite_block(x, ratio=16):\n    f = x.shape[-1]\n    se = layers.GlobalAveragePooling2D()(x)\n    se = layers.Reshape((1,1,f))(se)\n    se = layers.Dense(f//ratio, activation='relu')(se)\n    se = layers.Dense(f, activation='sigmoid')(se)\n    return layers.multiply([x,se])\n\ndef build_student_model():\n    inp = Input((*IMG_SIZE,3))\n    x = layers.Conv2D(32,3,padding='same',activation='relu',\n                      kernel_regularizer=regularizers.l2(1e-4))(inp)\n    x = layers.BatchNormalization()(x); x = layers.MaxPool2D()(x)\n\n    x = layers.SeparableConv2D(64,3,padding='same',activation='relu',\n        depthwise_regularizer=regularizers.l2(1e-4),pointwise_regularizer=regularizers.l2(1e-4))(x)\n    x = layers.BatchNormalization()(x); x = squeeze_excite_block(x); x = layers.MaxPool2D()(x)\n\n    x = layers.Conv2D(128,3,padding='same',activation='relu',\n                      kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = layers.BatchNormalization()(x); x = layers.MaxPool2D()(x); x = layers.Dropout(0.3)(x)\n\n    x = layers.Conv2D(256,3,padding='same',activation='relu',\n                      kernel_regularizer=regularizers.l2(1e-4))(x)\n    x = layers.BatchNormalization()(x)\n    feat = layers.GlobalAveragePooling2D()(x)\n    pred = layers.Dense(len(classes), activation='softmax')(feat)\n\n    return Model(inp, [feat, pred])\n    \nclass MultiLevelDistiller(tf.keras.Model):\n    def __init__(self, student, teachers, temp=5., alpha=0.5, beta=0.5):\n        super().__init__()\n        self.student = student\n        self.teachers = teachers\n        self.temp, self.alpha, self.beta = temp, alpha, beta\n        self.ce  = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n        self.kld = tf.keras.losses.KLDivergence()\n        self.mse = tf.keras.losses.MeanSquaredError()\n        self.acc = tf.keras.metrics.CategoricalAccuracy()\n        \n        self.w = self.add_weight(name=\"tw\", shape=(len(teachers),),\n                                 initializer=\"ones\", trainable=True)\n        \n        self.proj = layers.Dense(PROJ_DIM, activation='relu', name='student_proj')\n\n    def compile(self, opt):\n        super().compile()\n        self.opt = opt\n\n    def train_step(self, data):\n        x, y = data\n        t_feats, t_preds = [], []\n        for t in self.teachers:\n            f, p = t(x, training=False)\n            t_feats.append(f)\n            t_preds.append(p)\n\n        weights = tf.nn.softmax(self.w)\n        t_preds_stack = tf.stack(t_preds)\n        t_feats_stack = tf.stack(t_feats)\n        weights_reshaped = tf.reshape(weights, (-1, 1, 1))\n\n        ft = tf.reduce_sum(weights_reshaped * t_preds_stack, axis=0)\n        ff = tf.reduce_sum(weights_reshaped * t_feats_stack, axis=0)\n\n        with tf.GradientTape() as tape:\n            s_feat, s_pred = self.student(x, training=True)\n            s_proj = self.proj(s_feat)\n\n            loss_ce = self.ce(y, s_pred)\n            loss_kd = self.kld(tf.nn.softmax(ft / self.temp), tf.nn.softmax(s_pred / self.temp)) * self.temp**2\n            loss_feat = self.mse(ff, s_proj)\n\n            loss = self.alpha * loss_ce + (1 - self.alpha) * (loss_kd + self.beta * loss_feat)\n\n        vars = self.student.trainable_variables + self.proj.trainable_variables + [self.w]\n        grads = tape.gradient(loss, vars)\n        self.opt.apply_gradients(zip(grads, vars))\n\n        self.acc.update_state(y, s_pred)\n        return {\"loss\": loss, \"accuracy\": self.acc.result()}\n\n    def test_step(self, data):\n        x, y = data\n        _, s_pred = self.student(x, training=False)\n        loss = self.ce(y, s_pred)\n        self.acc.update_state(y, s_pred)\n        return {\"loss\": loss, \"accuracy\": self.acc.result()}\n\n\n# -----------------------------\n# 3. K‑FOLD CROSS‑VALIDATION\n# -----------------------------\nkf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\ncv_acc = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(file_paths, labels), 1):\n    print(f\"\\n=== Fold {fold}/{N_SPLITS} ===\")\n\n    x_tr, y_tr = file_paths[train_idx], labels[train_idx]\n    x_vl, y_vl = file_paths[val_idx], labels[val_idx]\n\n    y_tr_cat = tf.keras.utils.to_categorical(y_tr, num_classes=len(classes))\n    y_vl_cat = tf.keras.utils.to_categorical(y_vl, num_classes=len(classes))\n\n    y_tr_str = [classes[i] for i in y_tr]\n    y_vl_str = [classes[i] for i in y_vl]\n\n    train_df = pd.DataFrame({\"filename\": x_tr, \"class\": y_tr_str})\n    val_df = pd.DataFrame({\"filename\": x_vl, \"class\": y_vl_str})\n\n    train_datagen = ImageDataGenerator(\n        rescale=1./255,\n        rotation_range=30,\n        horizontal_flip=True,\n        zoom_range=0.3,\n        width_shift_range=0.3,\n        height_shift_range=0.3,\n        brightness_range=[0.7,1.3],\n        shear_range=0.2,\n        fill_mode='nearest'\n    )\n    val_datagen = ImageDataGenerator(rescale=1./255)\n\n    train_gen = train_datagen.flow_from_dataframe(\n        train_df, x_col=\"filename\", y_col=\"class\",\n        target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n        class_mode=\"categorical\", shuffle=True\n    )\n    val_gen = val_datagen.flow_from_dataframe(\n        val_df, x_col=\"filename\", y_col=\"class\",\n        target_size=IMG_SIZE, batch_size=BATCH_SIZE,\n        class_mode=\"categorical\", shuffle=False\n    )\n\n    teachers = [\n        build_teacher_model(tf.keras.applications.MobileNetV3Large),\n        build_teacher_model(tf.keras.applications.EfficientNetB1),\n        build_teacher_model(tf.keras.applications.MobileNetV2),\n    ]\n    for t in teachers: t.trainable = False\n    student = build_student_model()\n\n    distiller = MultiLevelDistiller(student, teachers)\n    lr_sched = tf.keras.optimizers.schedules.CosineDecay(INITIAL_LR, decay_steps=10000, alpha=1e-6)\n    opt = tf.keras.optimizers.AdamW(learning_rate=lr_sched, weight_decay=1e-4)\n    distiller.compile(opt)\n\n    callbacks = [\n        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6, verbose=1),\n        tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=1),\n    ]\n\n    distiller.fit(train_gen, validation_data=val_gen, epochs=EPOCHS, callbacks=callbacks, verbose=2)\n\n    res = distiller.evaluate(val_gen, verbose=0)\n    print(f\"Fold {fold} Val accuracy: {res[1]:.4f}\")\n    cv_acc.append(res[1])\n\n# -----------------------------\n# 4. FINAL RESULTS\n# -----------------------------\nprint(f\"\\nAverage CV Accuracy over {N_SPLITS} folds: {np.mean(cv_acc):.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T15:33:39.545909Z","iopub.execute_input":"2025-04-22T15:33:39.546524Z","iopub.status.idle":"2025-04-22T20:01:05.637195Z","shell.execute_reply.started":"2025-04-22T15:33:39.546501Z","shell.execute_reply":"2025-04-22T20:01:05.636536Z"}},"outputs":[{"name":"stdout","text":"Total images: 58464\nClasses: ['Apple_Apple Scab', 'Apple_Black Rot', 'Apple_Cedar Apple Rust', 'Apple_Healthy', 'Bell Pepper_Bacterial Spot', 'Bell Pepper_Healthy', 'Cherry_Healthy', 'Cherry_Powdery Mildew', 'Corn (Maize)_Cercospora Leaf Spot', 'Corn (Maize)_Common Rust ', 'Corn (Maize)_Healthy', 'Corn (Maize)_Northern Leaf Blight', 'Grape_Black Rot', 'Grape_Esca (Black Measles)', 'Grape_Healthy', 'Grape_Leaf Blight', 'Peach_Bacterial Spot', 'Peach_Healthy', 'Potato_Early Blight', 'Potato_Healthy', 'Potato_Late Blight', 'Strawberry_Healthy', 'Strawberry_Leaf Scorch', 'Tomato_Bacterial Spot', 'Tomato_Early Blight', 'Tomato_Healthy', 'Tomato_Late Blight', 'Tomato_Septoria Leaf Spot', 'Tomato_Yellow Leaf Curl Virus']\n\n=== Fold 1/5 ===\nFound 46771 validated image filenames belonging to 29 classes.\nFound 11693 validated image filenames belonging to 29 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['tw'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"1462/1462 - 286s - 196ms/step - accuracy: 0.7588 - loss: 0.5229 - val_accuracy: 0.7834 - val_loss: 1.3796 - learning_rate: 0.0010\nEpoch 2/50\n1462/1462 - 240s - 164ms/step - accuracy: 0.9257 - loss: 0.5189 - val_accuracy: 0.8627 - val_loss: 1.0158 - learning_rate: 0.0010\nEpoch 3/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9531 - loss: 0.4547 - val_accuracy: 0.8915 - val_loss: 0.8957 - learning_rate: 0.0010\nEpoch 4/50\n1462/1462 - 236s - 162ms/step - accuracy: 0.9674 - loss: 0.4365 - val_accuracy: 0.9156 - val_loss: 0.7503 - learning_rate: 0.0010\nEpoch 5/50\n1462/1462 - 237s - 162ms/step - accuracy: 0.9751 - loss: 0.4117 - val_accuracy: 0.9406 - val_loss: 0.7226 - learning_rate: 0.0010\nEpoch 6/50\n1462/1462 - 236s - 161ms/step - accuracy: 0.9804 - loss: 0.4379 - val_accuracy: 0.9737 - val_loss: 0.7220 - learning_rate: 0.0010\nEpoch 7/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9833 - loss: 0.4144 - val_accuracy: 0.9726 - val_loss: 0.7348 - learning_rate: 0.0010\nEpoch 8/50\n1462/1462 - 235s - 160ms/step - accuracy: 0.9817 - loss: 0.3877 - val_accuracy: 0.9728 - val_loss: 0.7370 - learning_rate: 0.0010\nEpoch 9/50\n\nEpoch 9: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n1462/1462 - 232s - 158ms/step - accuracy: 0.9837 - loss: 0.4062 - val_accuracy: 0.9722 - val_loss: 0.7404 - learning_rate: 0.0010\nEpoch 10/50\n1462/1462 - 232s - 158ms/step - accuracy: 0.9832 - loss: 0.4298 - val_accuracy: 0.9734 - val_loss: 0.7340 - learning_rate: 5.0000e-04\nEpoch 11/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9827 - loss: 0.4087 - val_accuracy: 0.9728 - val_loss: 0.7353 - learning_rate: 5.0000e-04\nEpoch 11: early stopping\nRestoring model weights from the end of the best epoch: 6.\nFold 1 Val accuracy: 0.9737\n\n=== Fold 2/5 ===\nFound 46771 validated image filenames belonging to 29 classes.\nFound 11693 validated image filenames belonging to 29 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['tw'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"1462/1462 - 281s - 192ms/step - accuracy: 0.7286 - loss: 0.5588 - val_accuracy: 0.7499 - val_loss: 1.5337 - learning_rate: 0.0010\nEpoch 2/50\n1462/1462 - 237s - 162ms/step - accuracy: 0.9158 - loss: 0.4330 - val_accuracy: 0.9009 - val_loss: 1.0135 - learning_rate: 0.0010\nEpoch 3/50\n1462/1462 - 233s - 160ms/step - accuracy: 0.9495 - loss: 0.4621 - val_accuracy: 0.9524 - val_loss: 0.7505 - learning_rate: 0.0010\nEpoch 4/50\n1462/1462 - 232s - 159ms/step - accuracy: 0.9649 - loss: 0.4814 - val_accuracy: 0.9666 - val_loss: 0.7599 - learning_rate: 0.0010\nEpoch 5/50\n1462/1462 - 233s - 159ms/step - accuracy: 0.9764 - loss: 0.3878 - val_accuracy: 0.9757 - val_loss: 0.7442 - learning_rate: 0.0010\nEpoch 6/50\n1462/1462 - 232s - 159ms/step - accuracy: 0.9798 - loss: 0.3953 - val_accuracy: 0.9819 - val_loss: 0.7113 - learning_rate: 0.0010\nEpoch 7/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9829 - loss: 0.3793 - val_accuracy: 0.9810 - val_loss: 0.7090 - learning_rate: 0.0010\nEpoch 8/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9829 - loss: 0.4076 - val_accuracy: 0.9815 - val_loss: 0.7098 - learning_rate: 0.0010\nEpoch 9/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9824 - loss: 0.4020 - val_accuracy: 0.9820 - val_loss: 0.7093 - learning_rate: 0.0010\nEpoch 10/50\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n1462/1462 - 236s - 161ms/step - accuracy: 0.9830 - loss: 0.4055 - val_accuracy: 0.9814 - val_loss: 0.7095 - learning_rate: 0.0010\nEpoch 11/50\n1462/1462 - 236s - 161ms/step - accuracy: 0.9832 - loss: 0.3925 - val_accuracy: 0.9815 - val_loss: 0.7088 - learning_rate: 5.0000e-04\nEpoch 12/50\n1462/1462 - 236s - 161ms/step - accuracy: 0.9828 - loss: 0.4035 - val_accuracy: 0.9816 - val_loss: 0.7115 - learning_rate: 5.0000e-04\nEpoch 13/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9826 - loss: 0.3920 - val_accuracy: 0.9814 - val_loss: 0.7097 - learning_rate: 5.0000e-04\nEpoch 14/50\n\nEpoch 14: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n1462/1462 - 236s - 162ms/step - accuracy: 0.9819 - loss: 0.4168 - val_accuracy: 0.9809 - val_loss: 0.7093 - learning_rate: 5.0000e-04\nEpoch 15/50\n1462/1462 - 238s - 163ms/step - accuracy: 0.9831 - loss: 0.4317 - val_accuracy: 0.9814 - val_loss: 0.7057 - learning_rate: 2.5000e-04\nEpoch 16/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9826 - loss: 0.3918 - val_accuracy: 0.9814 - val_loss: 0.7081 - learning_rate: 2.5000e-04\nEpoch 17/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9830 - loss: 0.4078 - val_accuracy: 0.9811 - val_loss: 0.7069 - learning_rate: 2.5000e-04\nEpoch 18/50\n\nEpoch 18: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n1462/1462 - 234s - 160ms/step - accuracy: 0.9831 - loss: 0.4129 - val_accuracy: 0.9814 - val_loss: 0.7070 - learning_rate: 2.5000e-04\nEpoch 19/50\n1462/1462 - 233s - 159ms/step - accuracy: 0.9826 - loss: 0.3890 - val_accuracy: 0.9806 - val_loss: 0.7092 - learning_rate: 1.2500e-04\nEpoch 20/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9833 - loss: 0.4001 - val_accuracy: 0.9814 - val_loss: 0.7082 - learning_rate: 1.2500e-04\nEpoch 20: early stopping\nRestoring model weights from the end of the best epoch: 15.\nFold 2 Val accuracy: 0.9814\n\n=== Fold 3/5 ===\nFound 46771 validated image filenames belonging to 29 classes.\nFound 11693 validated image filenames belonging to 29 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['tw'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"1462/1462 - 282s - 193ms/step - accuracy: 0.7536 - loss: 0.4994 - val_accuracy: 0.8562 - val_loss: 1.0344 - learning_rate: 0.0010\nEpoch 2/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9280 - loss: 0.4393 - val_accuracy: 0.8844 - val_loss: 0.9458 - learning_rate: 0.0010\nEpoch 3/50\n1462/1462 - 233s - 160ms/step - accuracy: 0.9544 - loss: 0.4360 - val_accuracy: 0.8693 - val_loss: 1.5642 - learning_rate: 0.0010\nEpoch 4/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9689 - loss: 0.4611 - val_accuracy: 0.9572 - val_loss: 0.8123 - learning_rate: 0.0010\nEpoch 5/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9769 - loss: 0.4229 - val_accuracy: 0.9689 - val_loss: 0.8131 - learning_rate: 0.0010\nEpoch 6/50\n1462/1462 - 232s - 159ms/step - accuracy: 0.9819 - loss: 0.4190 - val_accuracy: 0.9731 - val_loss: 0.7987 - learning_rate: 0.0010\nEpoch 7/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9841 - loss: 0.3697 - val_accuracy: 0.9812 - val_loss: 0.7794 - learning_rate: 0.0010\nEpoch 8/50\n1462/1462 - 233s - 160ms/step - accuracy: 0.9839 - loss: 0.3783 - val_accuracy: 0.9812 - val_loss: 0.7794 - learning_rate: 0.0010\nEpoch 9/50\n1462/1462 - 236s - 161ms/step - accuracy: 0.9837 - loss: 0.3727 - val_accuracy: 0.9813 - val_loss: 0.7843 - learning_rate: 0.0010\nEpoch 10/50\n\nEpoch 10: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n1462/1462 - 234s - 160ms/step - accuracy: 0.9839 - loss: 0.4202 - val_accuracy: 0.9808 - val_loss: 0.7866 - learning_rate: 0.0010\nEpoch 11/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9842 - loss: 0.3645 - val_accuracy: 0.9806 - val_loss: 0.7850 - learning_rate: 5.0000e-04\nEpoch 12/50\n1462/1462 - 237s - 162ms/step - accuracy: 0.9836 - loss: 0.3792 - val_accuracy: 0.9805 - val_loss: 0.7795 - learning_rate: 5.0000e-04\nEpoch 13/50\n\nEpoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n1462/1462 - 240s - 164ms/step - accuracy: 0.9840 - loss: 0.3898 - val_accuracy: 0.9809 - val_loss: 0.7840 - learning_rate: 5.0000e-04\nEpoch 13: early stopping\nRestoring model weights from the end of the best epoch: 8.\nFold 3 Val accuracy: 0.9812\n\n=== Fold 4/5 ===\nFound 46771 validated image filenames belonging to 29 classes.\nFound 11693 validated image filenames belonging to 29 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['tw'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"1462/1462 - 281s - 192ms/step - accuracy: 0.7598 - loss: 0.5096 - val_accuracy: 0.8061 - val_loss: 1.0012 - learning_rate: 0.0010\nEpoch 2/50\n1462/1462 - 232s - 159ms/step - accuracy: 0.9304 - loss: 0.4105 - val_accuracy: 0.9316 - val_loss: 0.9046 - learning_rate: 0.0010\nEpoch 3/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9569 - loss: 0.4365 - val_accuracy: 0.9174 - val_loss: 0.9014 - learning_rate: 0.0010\nEpoch 4/50\n1462/1462 - 240s - 164ms/step - accuracy: 0.9706 - loss: 0.4244 - val_accuracy: 0.9737 - val_loss: 0.8783 - learning_rate: 0.0010\nEpoch 5/50\n1462/1462 - 237s - 162ms/step - accuracy: 0.9772 - loss: 0.4060 - val_accuracy: 0.9677 - val_loss: 0.7503 - learning_rate: 0.0010\nEpoch 6/50\n1462/1462 - 241s - 165ms/step - accuracy: 0.9829 - loss: 0.4107 - val_accuracy: 0.9789 - val_loss: 0.7688 - learning_rate: 0.0010\nEpoch 7/50\n1462/1462 - 237s - 162ms/step - accuracy: 0.9838 - loss: 0.4441 - val_accuracy: 0.9780 - val_loss: 0.7586 - learning_rate: 0.0010\nEpoch 8/50\n\nEpoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n1462/1462 - 238s - 163ms/step - accuracy: 0.9851 - loss: 0.4096 - val_accuracy: 0.9781 - val_loss: 0.7559 - learning_rate: 0.0010\nEpoch 9/50\n1462/1462 - 239s - 163ms/step - accuracy: 0.9839 - loss: 0.3772 - val_accuracy: 0.9787 - val_loss: 0.7569 - learning_rate: 5.0000e-04\nEpoch 10/50\n1462/1462 - 233s - 160ms/step - accuracy: 0.9851 - loss: 0.3972 - val_accuracy: 0.9774 - val_loss: 0.7566 - learning_rate: 5.0000e-04\nEpoch 10: early stopping\nRestoring model weights from the end of the best epoch: 5.\nFold 4 Val accuracy: 0.9677\n\n=== Fold 5/5 ===\nFound 46772 validated image filenames belonging to 29 classes.\nFound 11692 validated image filenames belonging to 29 classes.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/applications/mobilenet_v3.py:517: UserWarning: `input_shape` is undefined or non-square, or `rows` is not 224. Weights for input shape (224, 224) will be loaded as the default.\n  return MobileNetV3(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:678: UserWarning: Gradients do not exist for variables ['tw'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"1462/1462 - 286s - 195ms/step - accuracy: 0.7600 - loss: 0.5421 - val_accuracy: 0.8576 - val_loss: 0.8504 - learning_rate: 0.0010\nEpoch 2/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9299 - loss: 0.4390 - val_accuracy: 0.9188 - val_loss: 0.7948 - learning_rate: 0.0010\nEpoch 3/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9554 - loss: 0.5009 - val_accuracy: 0.9589 - val_loss: 0.7898 - learning_rate: 0.0010\nEpoch 4/50\n1462/1462 - 232s - 159ms/step - accuracy: 0.9696 - loss: 0.4142 - val_accuracy: 0.9302 - val_loss: 0.8023 - learning_rate: 0.0010\nEpoch 5/50\n1462/1462 - 236s - 161ms/step - accuracy: 0.9776 - loss: 0.3897 - val_accuracy: 0.9674 - val_loss: 0.7986 - learning_rate: 0.0010\nEpoch 6/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9829 - loss: 0.4142 - val_accuracy: 0.9835 - val_loss: 0.7671 - learning_rate: 0.0010\nEpoch 7/50\n1462/1462 - 233s - 159ms/step - accuracy: 0.9850 - loss: 0.4210 - val_accuracy: 0.9828 - val_loss: 0.7686 - learning_rate: 0.0010\nEpoch 8/50\n1462/1462 - 231s - 158ms/step - accuracy: 0.9852 - loss: 0.4056 - val_accuracy: 0.9826 - val_loss: 0.7638 - learning_rate: 0.0010\nEpoch 9/50\n1462/1462 - 233s - 159ms/step - accuracy: 0.9848 - loss: 0.4184 - val_accuracy: 0.9827 - val_loss: 0.7678 - learning_rate: 0.0010\nEpoch 10/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9856 - loss: 0.4086 - val_accuracy: 0.9823 - val_loss: 0.7669 - learning_rate: 0.0010\nEpoch 11/50\n\nEpoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n1462/1462 - 233s - 159ms/step - accuracy: 0.9851 - loss: 0.3806 - val_accuracy: 0.9830 - val_loss: 0.7650 - learning_rate: 0.0010\nEpoch 12/50\n1462/1462 - 235s - 161ms/step - accuracy: 0.9856 - loss: 0.3885 - val_accuracy: 0.9832 - val_loss: 0.7684 - learning_rate: 5.0000e-04\nEpoch 13/50\n1462/1462 - 234s - 160ms/step - accuracy: 0.9850 - loss: 0.3941 - val_accuracy: 0.9829 - val_loss: 0.7692 - learning_rate: 5.0000e-04\nEpoch 13: early stopping\nRestoring model weights from the end of the best epoch: 8.\nFold 5 Val accuracy: 0.9826\n\nAverage CV Accuracy over 5 folds: 0.9773\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import os\n\nfinal_model_save_path = \"/kaggle/working/final_model\"\nos.makedirs(final_model_save_path, exist_ok=True)\n\n# Save the trained student model from the distiller (after final fold completion)\nstudent_model = distiller.student\n\n# Save the student model\nstudent_model.save(os.path.join(final_model_save_path, \"student_model.h5\"))\n\ndistiller.save(os.path.join(final_model_save_path, \"distiller_model.h5\"))\n\nprint(f\"Model saved at {final_model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T20:01:13.786588Z","iopub.execute_input":"2025-04-22T20:01:13.786858Z","iopub.status.idle":"2025-04-22T20:01:14.507537Z","shell.execute_reply.started":"2025-04-22T20:01:13.786837Z","shell.execute_reply":"2025-04-22T20:01:14.506740Z"}},"outputs":[{"name":"stdout","text":"Model saved at /kaggle/working/final_model\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}